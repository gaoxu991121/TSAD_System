{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a026ab01-0d5c-4d5d-9cbe-f94b49f47986",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from importlib import import_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from Preprocess.Normalization import minMaxNormalization\n",
    "from Preprocess.Window import convertToSlidingWindow\n",
    "from Utils.DataUtil import readData\n",
    "from Utils.DistanceUtil import KLDivergence, Softmax, JSDivergence\n",
    "from Utils.EvalUtil import findSegment, countResult\n",
    "from Utils.LogUtil import wirteLog\n",
    "from Utils.PlotUtil import plotAllResult\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575a7d59-177a-49a5-96fc-5421aa348eec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f4f093-cdc6-4798-82d8-4dcda9f2b2f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def calculateSimilarity(origin_sample_list,new_sample_list,old_anomaly_scores,old_label_samples,threshold = 0.5):\n",
    "#\n",
    "#     '''\n",
    "#     计算新数据列表和旧数据列表的相似性，返回列表\n",
    "#     :param origin_sample_list: 需要比较的旧数据的样本列表,即窗口列表\n",
    "#     :param new_sample_list: 需要比较的新数据的样本列表,即窗口列表\n",
    "#     :return:返回列表格式，每个新数据样本对应的相似性最大的旧数据样本的Index以及相似性数值。 [(max_similarity_index,max_similarity)]\n",
    "#     '''\n",
    "#\n",
    "#     total_similarity = 0\n",
    "#\n",
    "#     result = []\n",
    "#     for new_index,new_sample in enumerate(new_sample_list):\n",
    "#         max_similarity = 0\n",
    "#         max_similarity_index = 0\n",
    "#         for origin_index,origin_sample in enumerate(origin_sample_list):\n",
    "#\n",
    "#             similarity = getSimilarity(origin_sample,new_sample)\n",
    "#             if similarity > max_similarity:\n",
    "#                 max_similarity = similarity\n",
    "#                 max_similarity_index = origin_index\n",
    "#\n",
    "#         total_similarity += max_similarity\n",
    "#\n",
    "#         result.append((max_similarity_index,max_similarity))\n",
    "#\n",
    "#     return result,total_similarity\n",
    "\n",
    "\n",
    "def getMatrixKey(sample):\n",
    "    first = np.mean(sample[0])\n",
    "    last = np.mean(sample[-1])\n",
    "    mean_all = np.mean(sample)\n",
    "    var_all = np.var(sample)\n",
    "\n",
    "    mean_all= np.floor(mean_all * 100)   # 先乘以10，再使用floor，然后再除以10\n",
    "    var_all = np.floor(var_all * 100)\n",
    "    last  = np.floor(last * 100)\n",
    "    first = np.floor(first * 100)\n",
    "    res = f\"{mean_all}{var_all}{last}{first}\"\n",
    "    return res.replace(\".\",\"-\")\n",
    "\n",
    "\n",
    "def getDistinctAndNum(sample_all) -> dict:\n",
    "\n",
    "    result = {}\n",
    "    for new_sample in sample_all:\n",
    "        # new_sample_flatten = new_sample.flatten()\n",
    "        key = getMatrixKey(new_sample)\n",
    "        if result.get(key) == None:\n",
    "            result[key] = countSame(new_sample,sample_all)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unique(array_list):\n",
    "    # 获取数组形状\n",
    "    unique_arrays = {tuple(map(tuple, array)): array for array in array_list}\n",
    "\n",
    "    # 提取去重后的 NumPy 数组\n",
    "    unique_array_list = list(unique_arrays.values())\n",
    "\n",
    "    return unique_array_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7955e0ce-1eda-4481-bf27-bc712156a9a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convertToWindow(data, window_size):\n",
    "    \"\"\"\n",
    "    stride为1，前window_size -1 个时间点的时间窗口，通过复制前面元素构成\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "\n",
    "    for i, g in enumerate(data):\n",
    "        if i >= window_size:\n",
    "            w = data[i - window_size + 1:i + 1]\n",
    "        else:\n",
    "\n",
    "            w = np.concatenate([np.tile(data[0], window_size - i).reshape(window_size - i, -1), data[1:i + 1]])\n",
    "\n",
    "        windows.append(w)\n",
    "    return np.stack(windows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af954d2-061f-4075-ada6-b761e86d9e33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getConfigs():\n",
    "    config = {\n",
    "            \"epoch\": 2,\n",
    "            \"batch_size\": 128,\n",
    "            \"window_size\": 10,\n",
    "            \"identifier\": \"model-evaluation\",\n",
    "            \"hidden_size\": 64,\n",
    "            \"latent_size\": 32,\n",
    "            \"num_layers\": 2,\n",
    "            \"num_heads\": 1,\n",
    "            \"drop_out_rate\": 0.1,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"patience\": 10,\n",
    "            \"mask\": False,\n",
    "            \"lambda_energy\": 0.1,\n",
    "            \"lambda_cov_diag\": 0.005,\n",
    "\n",
    "            \"num_filters\":3,\n",
    "            \"kernel_size\":3,\n",
    "\n",
    "            \"explained_var\":0.9,\n",
    "\n",
    "            \"kernel\": \"rbf\",\n",
    "            \"gamma\": \"auto\",\n",
    "            \"degree\": 3,\n",
    "            \"coef0\": 0.0,\n",
    "            \"tol\": 0.001,\n",
    "            \"cache_size\": 200,\n",
    "            \"shrinking\": True,\n",
    "            \"nu\": 0.48899475599830133,\n",
    "            \"step_max\": 5,\n",
    "\n",
    "            \"n_trees\": 100,\n",
    "            \"max_samples\": \"auto\",\n",
    "            \"max_features\": 1,\n",
    "            \"bootstrap\": False,\n",
    "            \"random_state\": 42,\n",
    "            \"verbose\": 0,\n",
    "            \"n_jobs\": 1,\n",
    "            \"contamination\": 0.5,\n",
    "\n",
    "\n",
    "            \"nz\":10,\n",
    "            \"beta\":0.5\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    return config\n",
    "def getModel(config):\n",
    "    method = config[\"model_name\"]\n",
    "    module = import_module(\"Models.\"+method+\".Model\")\n",
    "    # 获取类引用\n",
    "    clazz = getattr(module, method)\n",
    "\n",
    "    # 创建类的实例\n",
    "    model = clazz(config).float()\n",
    "    # model = model_dict[method].Model(args).float()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def getDatasetSplitConfig():\n",
    "    config = {\n",
    "        \"SKAB\":26322,\n",
    "        \"PMS\":53122,\n",
    "        \"DMDS\":200000,\n",
    "        \"WADI\":130000,\n",
    "        \"SWAT\":155000,\n",
    "\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def checkHolderExist(path):\n",
    "    # 判断文件夹是否存在\n",
    "    if not os.path.exists(path):\n",
    "        # 如果文件夹不存在，则创建它\n",
    "        os.makedirs(path)\n",
    "\n",
    "def splitFiles(files):\n",
    "    random.shuffle(files)\n",
    "    split_index = len(files) // 3\n",
    "    return files[:split_index], files[split_index:]\n",
    "\n",
    "def convertRecToWindow(dataset = \"WADI\",window_size = 100):\n",
    "    # 分割出新旧数据后，转变数据为滑动窗口\n",
    "    mode = \"old\"\n",
    "    recom_dataset_path = \"./RecomData/\" + mode + \"/\" + dataset\n",
    "    data_files = os.listdir(recom_dataset_path + \"/train\")\n",
    "    for file in data_files:\n",
    "        writeWindowDataset(base_path=recom_dataset_path, filename=file, window_size=window_size)\n",
    "\n",
    "    mode = \"new\"\n",
    "    recom_dataset_path = \"./RecomData/\" + mode + \"/\" + dataset\n",
    "    data_files = os.listdir(recom_dataset_path + \"/train\")\n",
    "    for file in data_files:\n",
    "        writeWindowDataset(base_path=recom_dataset_path, filename=file, window_size=window_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c9c74-d6f4-4fe6-bfc4-77c04c3aa148",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41367e13-ad5a-4b76-bb5e-6f7e8cd27cee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def processWADI(dataset,step):\n",
    "\n",
    "    dataset_split_config = getDatasetSplitConfig()\n",
    "    dataset_path = \"./Data/\" + dataset\n",
    "    if step == 1:\n",
    "\n",
    "\n",
    "        savepath_train_old = \"./RecomData/old/\" + dataset + \"/train\"\n",
    "        savepath_train_new = \"./RecomData/new/\" + dataset + \"/train\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        checkHolderExist(savepath_train_old)\n",
    "        checkHolderExist(savepath_train_new)\n",
    "\n",
    "\n",
    "\n",
    "        # 划分旧数据和新数据\n",
    "\n",
    "\n",
    "        data_train_path = dataset_path + \"/train/\" + dataset + \".csv\"\n",
    "\n",
    "\n",
    "\n",
    "        data_train = pd.read_csv(data_train_path, header=None).to_numpy()\n",
    "\n",
    "\n",
    "        data_train[np.isnan(data_train)] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        data_train = minMaxNormalization(data_train)\n",
    "\n",
    "\n",
    "        np.save(savepath_train_old + \"/\" + dataset + \".npy\", data_train)\n",
    "        np.save(savepath_train_new + \"/\" + dataset + \".npy\", data_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif step == 2:\n",
    "        data_test_path = dataset_path + \"/test/\" + dataset + \".csv\"\n",
    "        data_test = pd.read_csv(data_test_path, header=None).to_numpy()\n",
    "        data_test[np.isnan(data_test)] = 0\n",
    "\n",
    "        savepath_test_old = \"./RecomData/old/\" + dataset + \"/test\"\n",
    "        savepath_test_new = \"./RecomData/new/\" + dataset + \"/test\"\n",
    "\n",
    "        checkHolderExist(savepath_test_new)\n",
    "        checkHolderExist(savepath_test_old)\n",
    "\n",
    "        split_index = dataset_split_config[dataset]\n",
    "\n",
    "        old_data_test = data_test[:split_index, :]\n",
    "        new_data_test = data_test[split_index:, :]\n",
    "\n",
    "        old_data_test = minMaxNormalization(old_data_test)\n",
    "        new_data_test = minMaxNormalization(new_data_test)\n",
    "\n",
    "        np.save(savepath_test_old + \"/\" + dataset + \".npy\", old_data_test)\n",
    "        np.save(savepath_test_new + \"/\" + dataset + \".npy\", new_data_test)\n",
    "\n",
    "\n",
    "    elif step == 3:\n",
    "        savepath_label_old = \"./RecomData/old/\" + dataset + \"/label\"\n",
    "        savepath_label_new = \"./RecomData/new/\" + dataset + \"/label\"\n",
    "\n",
    "\n",
    "        checkHolderExist(savepath_label_old)\n",
    "        checkHolderExist(savepath_label_new)\n",
    "        data_label_path = dataset_path + \"/label/\" + dataset + \".csv\"\n",
    "        label = pd.read_csv(data_label_path, header=None).to_numpy().squeeze()\n",
    "        split_index = dataset_split_config[dataset]\n",
    "        old_label = label[:split_index]\n",
    "        new_label = label[split_index:]\n",
    "\n",
    "        np.save(savepath_label_old + \"/\" + dataset + \".npy\", old_label)\n",
    "        np.save(savepath_label_new + \"/\" + dataset + \".npy\", new_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def datasetProcess():\n",
    "    dataset_pair = [ (\"UCR\", False),  (\"SMD\", False), (\"SMAP\", False), (\"SKAB\", True),\n",
    "                   (\"PMS\", True), (\"MSL\", False), (\"DMDS\", True)]\n",
    "\n",
    "    config = getConfigs()\n",
    "\n",
    "    dataset_split_config = getDatasetSplitConfig()\n",
    "\n",
    "    window_size = config[\"window_size\"]\n",
    "\n",
    "    for dataset, onlyone in dataset_pair:\n",
    "        print(\"dataset:\",dataset)\n",
    "        dataset_path = \"./Data/\" + dataset\n",
    "\n",
    "        savepath_train_old = \"./RecomData/old/\" + dataset + \"/train\"\n",
    "        savepath_train_new = \"./RecomData/new/\" + dataset + \"/train\"\n",
    "\n",
    "\n",
    "        savepath_test_old = \"./RecomData/old/\" + dataset + \"/test\"\n",
    "        savepath_label_old = \"./RecomData/old/\" + dataset + \"/label\"\n",
    "\n",
    "        savepath_test_new = \"./RecomData/new/\" + dataset + \"/test\"\n",
    "        savepath_label_new = \"./RecomData/new/\" + dataset + \"/label\"\n",
    "\n",
    "\n",
    "\n",
    "        checkHolderExist(savepath_train_old)\n",
    "        checkHolderExist(savepath_train_new)\n",
    "        checkHolderExist(savepath_test_old)\n",
    "        checkHolderExist(savepath_label_old)\n",
    "        checkHolderExist(savepath_test_new)\n",
    "        checkHolderExist(savepath_label_new)\n",
    "\n",
    "        #划分旧数据和新数据\n",
    "\n",
    "        if onlyone:\n",
    "            data_train_path = dataset_path + \"/train/\" + dataset + \".csv\"\n",
    "            data_test_path = dataset_path + \"/test/\" + dataset + \".csv\"\n",
    "            data_label_path = dataset_path + \"/label/\" + dataset + \".csv\"\n",
    "\n",
    "\n",
    "\n",
    "            data_train = pd.read_csv(data_train_path, header=None).to_numpy()\n",
    "            data_test = pd.read_csv(data_test_path, header=None).to_numpy()\n",
    "\n",
    "            data_train[np.isnan(data_train)] = 0\n",
    "            data_test[np.isnan(data_test)] = 0\n",
    "\n",
    "\n",
    "            label = pd.read_csv(data_label_path, header=None).to_numpy().squeeze()\n",
    "\n",
    "\n",
    "            split_index = dataset_split_config[dataset]\n",
    "\n",
    "            old_data_test = data_test[:split_index,:]\n",
    "            new_data_test = data_test[split_index:, :]\n",
    "\n",
    "            old_label = label[:split_index]\n",
    "            new_label = label[split_index:]\n",
    "\n",
    "\n",
    "            data_train = minMaxNormalization(data_train)\n",
    "            old_data_test = minMaxNormalization(old_data_test)\n",
    "            new_data_test = minMaxNormalization(new_data_test)\n",
    "\n",
    "\n",
    "            np.save(savepath_train_old + \"/\" + dataset + \".npy\", data_train)\n",
    "            np.save(savepath_train_new + \"/\" + dataset + \".npy\", data_train)\n",
    "\n",
    "\n",
    "            np.save(savepath_test_old + \"/\" + dataset + \".npy\",  old_data_test)\n",
    "            np.save(savepath_test_new + \"/\" + dataset + \".npy\",  new_data_test)\n",
    "\n",
    "            np.save(savepath_label_old + \"/\" + dataset + \".npy\",  old_label)\n",
    "            np.save(savepath_label_new + \"/\" + dataset + \".npy\", new_label)\n",
    "\n",
    "            del data_train\n",
    "            del old_data_test\n",
    "            del new_data_test\n",
    "            del old_label\n",
    "            del new_label\n",
    "\n",
    "        else:\n",
    "\n",
    "            data_train_path = dataset_path + \"/train/\"\n",
    "            data_test_path = dataset_path + \"/test/\"\n",
    "            data_label_path = dataset_path + \"/label/\"\n",
    "\n",
    "\n",
    "\n",
    "            data_files = os.listdir(data_train_path)\n",
    "\n",
    "\n",
    "            #随机划分新旧数据\n",
    "            files_new, files_old = splitFiles(data_files)\n",
    "            for file in files_new:\n",
    "                try:\n",
    "                    data_train = pd.read_csv(os.path.join(data_train_path, file), header=None).to_numpy()\n",
    "                    data_test = pd.read_csv(os.path.join(data_test_path, file), header=None).to_numpy()\n",
    "\n",
    "                    data_train[np.isnan(data_train)] = 0\n",
    "                    data_test[np.isnan(data_test)] = 0\n",
    "\n",
    "\n",
    "                    label = pd.read_csv(os.path.join(data_label_path, file), header=None).to_numpy().squeeze()\n",
    "\n",
    "                    data_train = minMaxNormalization(data_train)\n",
    "                    data_test = minMaxNormalization(data_test)\n",
    "\n",
    "\n",
    "\n",
    "                    filename = file.split(\".\")[0]\n",
    "                    np.save(savepath_train_new + \"/\" + filename + \".npy\", data_train)\n",
    "                    np.save(savepath_test_new + \"/\" + filename + \".npy\", data_test)\n",
    "                    np.save(savepath_label_new + \"/\" + filename + \".npy\", label)\n",
    "                except Exception as e:\n",
    "                    # 打印错误信息并跳过该文件\n",
    "                    print(f\"Error occurred while processing file {file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            for file in files_old:\n",
    "                try:\n",
    "                    data_train = pd.read_csv(os.path.join(data_train_path, file), header=None).to_numpy()\n",
    "                    data_test = pd.read_csv(os.path.join(data_test_path, file), header=None).to_numpy()\n",
    "\n",
    "                    data_train[np.isnan(data_train)] = 0\n",
    "                    data_test[np.isnan(data_test)] = 0\n",
    "\n",
    "\n",
    "                    label = pd.read_csv(os.path.join(data_label_path, file), header=None).to_numpy().squeeze()\n",
    "\n",
    "                    data_train = minMaxNormalization(data_train)\n",
    "                    data_test = minMaxNormalization(data_test)\n",
    "\n",
    "\n",
    "                    filename = file.split(\".\")[0]\n",
    "                    np.save(savepath_train_old + \"/\" + filename + \".npy\", data_train)\n",
    "                    np.save(savepath_test_old + \"/\" + filename + \".npy\", data_test)\n",
    "                    np.save(savepath_label_old + \"/\" + filename + \".npy\", label)\n",
    "                except Exception as e:\n",
    "                    # 打印错误信息并跳过该文件\n",
    "                    print(f\"Error occurred while processing file {file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "        #分割出新旧数据后，转变数据为滑动窗口\n",
    "        mode = \"old\"\n",
    "        recom_dataset_path =  \"./RecomData/\" + mode +\"/\" + dataset\n",
    "        data_files = os.listdir(recom_dataset_path + \"/train\")\n",
    "        for file in data_files:\n",
    "            writeWindowDataset(base_path=recom_dataset_path,filename=file,window_size=window_size)\n",
    "\n",
    "\n",
    "        mode = \"new\"\n",
    "        recom_dataset_path = \"./RecomData/\" + mode + \"/\" + dataset\n",
    "        data_files = os.listdir(recom_dataset_path + \"/train\")\n",
    "        for file in data_files:\n",
    "            writeWindowDataset(base_path=recom_dataset_path, filename=file, window_size=window_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e82fad7-5dc8-45d6-9e0a-bfbfa735dad1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def writeWindowDataset(base_path,filename,window_size):\n",
    "    '''\n",
    "    针对单个地址转化窗口保存，window_size由config指定\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    data_train = np.load(base_path+\"/train/\"+filename)\n",
    "    data_test = np.load(base_path+\"/test/\"+filename)\n",
    "    label = np.load(base_path+\"/label/\"+filename)\n",
    "\n",
    "\n",
    "    train_window = convertToSlidingWindow(data_train, window_size=window_size)\n",
    "    test_window = convertToSlidingWindow(data_test, window_size=window_size)\n",
    "    label = label[window_size - 1:]\n",
    "\n",
    "    print(\"test_window shape:\",test_window.shape)\n",
    "    print(\"label shape:\",label.shape)\n",
    "\n",
    "    savepath_train = base_path + \"/window/train/\"\n",
    "    savepath_test = base_path + \"/window/test/\"\n",
    "    savepath_label = base_path + \"/window/label/\"\n",
    "    checkHolderExist(savepath_train)\n",
    "    checkHolderExist(savepath_test)\n",
    "    checkHolderExist(savepath_label)\n",
    "\n",
    "    np.save(savepath_train + \"/\" + filename , train_window)\n",
    "    np.save(savepath_test + \"/\" + filename , test_window)\n",
    "    np.save(savepath_label + \"/\" + filename , label)\n",
    "\n",
    "\n",
    "def evalOneDatasetFile(dataset_name,filename,mode = \"old\"):\n",
    "    config = getConfigs()\n",
    "    #model_list = [\"LSTMVAE\",\"LSTMAE\",\"NASALSTM\",\"DAGMM\",\"TRANSFORMER\",\"TCNAE\",\"UAE\",\"TRANAD\",\"OmniAnomaly\",\"PCAAD\",\"IForestAD\"]\n",
    "    model_list = [\"LSTMV2\"]\n",
    "    # model_list = [\"LSTMVAE\",\"PCAAD\"]\n",
    "    base_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    #get data\n",
    "    window_size = config[\"window_size\"]\n",
    "    data_train,data_test,label = readData(dataset_path = base_path + \"/RecomData/\" + mode + \"/\" + dataset_name ,filename = filename,file_type = \"npy\")\n",
    "    label = label[window_size - 1:]\n",
    "    print(\"data_train shape:\",data_train.shape)\n",
    "    print(\"data_test shape:\", data_test.shape)\n",
    "    print(\"label shape:\", label.shape)\n",
    "    input_dim = data_train.shape[-1]\n",
    "\n",
    "    config[\"device\"] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config[\"base_path\"] = base_path\n",
    "    config[\"input_size\"] = input_dim\n",
    "\n",
    "    for method in model_list:\n",
    "        config[\"model_name\"] = method\n",
    "\n",
    "        if method in [\"TRANSFORMER\",\"TRANAD\"]:\n",
    "            config[\"epoch\"] = 5\n",
    "        else:\n",
    "            config[\"epoch\"] = 2\n",
    "\n",
    "        print(\"training method:\",method)\n",
    "\n",
    "        model = getModel(config)\n",
    "\n",
    "        config[\"model_param_num\"] = count_parameters(model)\n",
    "        config[\"identifier\"] = dataset_name+\"-\"+method\n",
    "        config[\"train_start_time\"] = time.time()\n",
    "        # train model\n",
    "        model.fit(train_data = data_train,write_log=True)\n",
    "        config[\"train_end_time\"] = time.time()\n",
    "\n",
    "        print(\"finish training method:\",method,\" cost time:\",config[\"train_end_time\"] - config[\"train_start_time\"])\n",
    "\n",
    "        config[\"test_start_time\"] = time.time()\n",
    "        anomaly_scores = model.test(data_test)\n",
    "        config[\"test_end_time\"] = time.time()\n",
    "        ori_predict_labels, ori_f1, ori_threshold = model.getBestPredict(anomaly_score=anomaly_scores, n_thresholds=25,\n",
    "                                                             ground_truth_label=label,protocol=\"\")\n",
    "\n",
    "\n",
    "        apa_predict_labels, apa_f1, apa_threshold = model.getBestPredict(anomaly_score=anomaly_scores, n_thresholds=25,\n",
    "                                                                         ground_truth_label=label,\n",
    "                                                                         protocol=\"apa\")\n",
    "\n",
    "        pa_predict_labels, pa_f1, pa_threshold = model.getBestPredict(anomaly_score=anomaly_scores, n_thresholds=25,\n",
    "                                                                      ground_truth_label=label,\n",
    "                                                                      protocol=\"pa\")\n",
    "\n",
    "        (tp, fp, tn, fn) = countResult(predict_labels=ori_predict_labels, ground_truth=label)\n",
    "        config[\"ori_tp\"] = float(tp)\n",
    "        config[\"ori_fp\"] = float(fp)\n",
    "        config[\"ori_tn\"] = float(tn)\n",
    "        config[\"ori_fn\"] = float(fn)\n",
    "\n",
    "        print(\"finish evaluating method:\", method)\n",
    "        # visualization\n",
    "        plot_yaxis = []\n",
    "        plot_yaxis.append(anomaly_scores)\n",
    "        plot_yaxis.append(ori_predict_labels)\n",
    "        plot_yaxis.append(apa_predict_labels)\n",
    "        plot_yaxis.append(pa_predict_labels)\n",
    "\n",
    "        plot_path = base_path + \"/Plots/recommondation/\" + mode + \"/\" + dataset_name +\"/\" + filename\n",
    "\n",
    "        checkHolderExist(plot_path)\n",
    "\n",
    "        plotAllResult(x_axis=np.arange(len(anomaly_scores)), y_axises=plot_yaxis, title=config[\"model_name\"],\n",
    "                      save_path=plot_path + \"/\" + method + \".pdf\",\n",
    "                      segments=findSegment(label),\n",
    "                      threshold=None)\n",
    "\n",
    "        # config[\"anomaly_score\"] = anomaly_scores.tolist()\n",
    "        score_save_path = base_path + \"/RecomData/scores/\" + mode + \"/\"  + dataset_name + \"/\" + filename\n",
    "\n",
    "        checkHolderExist(score_save_path)\n",
    "        np.save(score_save_path + \"/\"  + method +\".npy\",anomaly_scores)\n",
    "        # config[\"ori_predict_labels\"] = ori_predict_labels.tolist()\n",
    "        # config[\"pa_predict_labels\"] = pa_predict_labels.tolist()\n",
    "        # config[\"apa_predict_labels\"] = apa_predict_labels.tolist()\n",
    "\n",
    "        config[\"ori_f1\"] = ori_f1\n",
    "        config[\"apa_f1\"] = apa_f1\n",
    "        config[\"pa_f1\"] = pa_f1\n",
    "\n",
    "        config[\"ori_threshold\"] = ori_threshold\n",
    "        config[\"apa_threshold\"] = apa_threshold\n",
    "        config[\"pa_threshold\"] = pa_threshold\n",
    "        config[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        wirteLog(base_path + \"/Logs/recommondation/\" + mode + \"/\"  + dataset_name + \"/\" + filename ,method,config)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"finish training model. start to test model.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49e50c6-f564-497e-ade7-ed743b614eee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sampleFromWindowData(data: np.ndarray,sample_num:int,indices:np.ndarray = np.array([])):\n",
    "    length = len(data)\n",
    "\n",
    "    results = []\n",
    "    if len(indices) == 0 :\n",
    "        # 计算均匀间隔\n",
    "        interval = length // sample_num\n",
    "        if interval < sample_num :\n",
    "            indices = np.random.choice(length, sample_num, replace=False)\n",
    "        else:\n",
    "            indexes = []\n",
    "            for i in range(sample_num):\n",
    "                idx = random.randint(i*interval,(i+1)*interval-1)\n",
    "                indexes.append(idx)\n",
    "            indices = indexes\n",
    "\n",
    "    for sample_index in indices:\n",
    "        results.append(data[sample_index])\n",
    "    \n",
    "    return results,indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8f825-dc10-48df-a64e-2739d2175236",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d96572a-81cc-4a0d-85a5-f766860eacb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getEvaluationResult(mode = \"old\",dataset_list = [],method_list = []):\n",
    "    path = \"./Logs/recommondation/\" + mode +\"/\"\n",
    "    result = {}\n",
    "    for dataset,isonly in dataset_list:\n",
    "\n",
    "        result[dataset] = {}\n",
    "\n",
    "        files_path = path + dataset\n",
    "\n",
    "        file_names = os.listdir(files_path)\n",
    "\n",
    "        for file_name in file_names:\n",
    "            file_name = file_name.split(\".\")[0]\n",
    "\n",
    "            result[dataset][file_name] = {}\n",
    "\n",
    "            for method in method_list:\n",
    "                result[dataset][file_name][method] = {}\n",
    "                eval_path = files_path + \"/\" + file_name + \"/\" + method + \".json\"\n",
    "                with open(eval_path, \"r\") as file:\n",
    "                    data_dict = json.load(file)\n",
    "\n",
    "                result[dataset][file_name][method][\"ori_f1\"] = data_dict[\"ori_f1\"]\n",
    "                result[dataset][file_name][method][\"pa_f1\"] = data_dict[\"pa_f1\"]\n",
    "                result[dataset][file_name][method][\"ori_f1\"] = data_dict[\"ori_f1\"]\n",
    "\n",
    "                result[dataset][file_name][method][\"pa_threshold\"] = data_dict[\"pa_threshold\"]\n",
    "                result[dataset][file_name][method][\"apa_threshold\"] = data_dict[\"apa_threshold\"]\n",
    "                result[dataset][file_name][method][\"ori_threshold\"] = data_dict[\"ori_threshold\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "def countSame(sample,all_sample):\n",
    "    count = np.sum(np.all(sample == all_sample, axis=(1, 2)))\n",
    "    return count\n",
    "\n",
    "def batchDiscretize(all_sample):\n",
    "    result = []\n",
    "    for item in all_sample:\n",
    "        result.append(discretize(item))\n",
    "    return result\n",
    "def discretize(data):\n",
    "    \"\"\"\n",
    "    将形状为[batch, window, channel]数值离散化\n",
    "    \"\"\"\n",
    "    # 创建一个存储离散化结果的新数组\n",
    "    data = np.floor(data * 10) / 10  # 先乘以10，再使用floor，然后再除以10\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f66ee50-b9a4-481e-8d22-fc1e4b8e4404",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getDatasetDetectability(origin_sample_list,new_sample_list,old_anomaly_scores,threshold,label_samples ,params):\n",
    "    total_dec = 0\n",
    "    for new_index, new_sample in enumerate(new_sample_list):\n",
    "        m_dec = getMatchScore(sample = new_sample,ori_sample_list = origin_sample_list,threshold = threshold,anomaly_score = old_anomaly_scores,label_samples = label_samples,params = params)\n",
    "        total_dec += m_dec\n",
    "    return total_dec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d495fa2-48a9-4bf7-bfeb-d46e4d34e2ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def getMatchScore(sample,ori_sample_list,threshold,anomaly_score,label_samples,params):\n",
    "#     m_dec = 0\n",
    "#     recall = params[\"recall\"]\n",
    "#     precision = params[\"precision\"]\n",
    "#     # ratio = params[\"anomaly_ratio\"]\n",
    "#     for index,ori_sample in enumerate(ori_sample_list):\n",
    "#         similarity = getSimilarity(sample, ori_sample)\n",
    "#         if similarity < threshold:\n",
    "#             m_dec +=  np.sum(  precision * np.multiply(label_samples[index],anomaly_score[index] ) +  recall * np.multiply((1 - label_samples[index]),(1-anomaly_score[index])) )\n",
    "\n",
    "#     return m_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e7d186-fc57-468b-ba53-ced60006a275",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getMatchScore(sample,ori_sample_list,threshold,anomaly_score,label_samples,params):\n",
    "    m_dec = 0\n",
    "    recall = params[\"recall\"] + 1e-6\n",
    "    precision = params[\"precision\"] + 1e-6\n",
    "    f1 = params[\"f1\"] + 1e-6\n",
    "    ratio = params[\"anomaly_ratio\"]\n",
    "    for index,ori_sample in enumerate(ori_sample_list):\n",
    "        similarity = getSimilarity(sample, ori_sample)\n",
    "        if similarity < threshold:\n",
    "            m_dec +=  np.sum( (1/(1-precision)) *  np.multiply(label_samples[index],anomaly_score[index] ) + ratio * recall * np.multiply((1 - label_samples[index]),(1-anomaly_score[index])) )\n",
    "\n",
    "    return m_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187e2334-137d-41af-b6e3-d27fc94d9363",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getDatasetSimilarity(origin_sample_list,new_sample_list,old_anomaly_scores,old_label_samples,threshold = 0.5,params = {}):\n",
    "\n",
    "    '''\n",
    "    计算新数据列表和旧数据列表的相似性，返回列表\n",
    "    :param origin_sample_list: 需要比较的旧数据的样本列表,即窗口列表\n",
    "    :param new_sample_list: 需要比较的新数据的样本列表,即窗口列表\n",
    "    :return:返回列表格式，每个新数据样本对应的相似性最大的旧数据样本的Index以及相似性数值。 [(max_similarity_index,max_similarity)]\n",
    "    '''\n",
    "    origin_sample_list = batchDiscretize(origin_sample_list)\n",
    "    new_sample_list = batchDiscretize(new_sample_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    new_counts_map = getDistinctAndNum(new_sample_list)\n",
    "\n",
    "    ori_len = len(origin_sample_list)\n",
    "    new_len = len(new_sample_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    new_sample_list = unique(new_sample_list)\n",
    "\n",
    "\n",
    "    p = np.zeros(len(new_sample_list))\n",
    "    for index,item in enumerate(new_sample_list):\n",
    "        key = getMatrixKey(item)\n",
    "        p[index] = new_counts_map[key]\n",
    "\n",
    "    p = Softmax(p + 1e-7)\n",
    "\n",
    "    q = np.zeros_like(p)\n",
    "\n",
    "    total_c = 0\n",
    "    c_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for new_index,new_sample in enumerate(new_sample_list):\n",
    "\n",
    "        counts,similar_sample_index_list = calculateSimilarityCounts(new_sample, origin_sample_list, threshold)\n",
    "        q[new_index] = counts\n",
    "        total_c += counts\n",
    "        c_list = list(set(c_list + similar_sample_index_list))\n",
    "\n",
    "    len_cd1 =  len(c_list)\n",
    "\n",
    "    q = q / (ori_len - len_cd1 + total_c )\n",
    "    q = Softmax(q + 1e-8)\n",
    "    dataset_similarity = 1 / (p * np.log(p/q) +1e-8).sum()\n",
    "\n",
    "    \n",
    "\n",
    "    m_dec_total = getDatasetDetectability(origin_sample_list, new_sample_list, old_anomaly_scores, threshold,old_label_samples,params)\n",
    "    #print(\"dataset_similarity = \", dataset_similarity,\"m_dec_total = \", m_dec_total )\n",
    "    total_similarity = dataset_similarity * m_dec_total\n",
    "\n",
    "    return total_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58385377-7811-411a-9911-1b4e5e30e9b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getEvalCount(mode=\"old\", dataset=\"\", method_list=[]):\n",
    "    path = \"./Logs/recommondation/\" + mode + \"/\"\n",
    "    result = {}\n",
    "\n",
    "    result[dataset] = {}\n",
    "\n",
    "    files_path = path + dataset\n",
    "\n",
    "    file_names = os.listdir(files_path)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        file_name = file_name.split(\".\")[0]\n",
    "\n",
    "        result[dataset][file_name] = {}\n",
    "\n",
    "        for method in method_list:\n",
    "            result[dataset][file_name][method] = {}\n",
    "            eval_path = files_path + \"/\" + file_name + \"/\" + method + \".json\"\n",
    "            with open(eval_path, \"r\") as file:\n",
    "                data_dict = json.load(file)\n",
    "\n",
    "            result[dataset][file_name][method][\"tp\"] = data_dict[\"ori_tp\"]\n",
    "            result[dataset][file_name][method][\"fp\"] = data_dict[\"ori_fp\"]\n",
    "            result[dataset][file_name][method][\"tn\"] = data_dict[\"ori_tn\"]\n",
    "            result[dataset][file_name][method][\"fn\"] = data_dict[\"ori_fn\"]\n",
    "            if (data_dict[\"ori_tp\"] + data_dict[\"ori_fp\"]) <= 0 :\n",
    "                result[dataset][file_name][method][\"precision\"] = 0\n",
    "            else:\n",
    "                result[dataset][file_name][method][\"precision\"] = data_dict[\"ori_tp\"] / (data_dict[\"ori_tp\"] + data_dict[\"ori_fp\"])\n",
    "\n",
    "            if (data_dict[\"ori_tp\"] + data_dict[\"ori_fn\"]) <= 0:\n",
    "                result[dataset][file_name][method][\"recall\"] = 0\n",
    "            else:\n",
    "                result[dataset][file_name][method][\"recall\"] = data_dict[\"ori_tp\"] / (data_dict[\"ori_tp\"] + data_dict[\"ori_fn\"])\n",
    "                \n",
    "\n",
    "            result[dataset][file_name][method][\"f1\"] = data_dict[\"ori_f1\"]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba15b0ec-dcd1-4fbc-86d8-431985dd627d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculateSimilarityCounts(sample,ori_sample_list,threshold):\n",
    "    counts = 0\n",
    "    similar_sample_index_list = []\n",
    "\n",
    "    for index,ori_sample in enumerate(ori_sample_list):\n",
    "        if ori_sample.shape != sample.shape:\n",
    "            print(\"shape dont match!:\",ori_sample.shape,sample.shape)\n",
    "        ori_sample = ori_sample.flatten()\n",
    "        \n",
    "        similarity = getSimilarity(sample.flatten() ,ori_sample)\n",
    "   \n",
    "        if similarity < threshold:\n",
    "            # print(\"similarity:\",similarity,\" threshold:\",threshold)\n",
    "            counts += 1\n",
    "            similar_sample_index_list.append(index)\n",
    "\n",
    "    return counts,similar_sample_index_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e07bd8-99fc-4a37-bd50-4ea2ed5a1e91",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getSimilarity(origin_sample,new_sample):\n",
    "    '''\n",
    "    具体计算相似性的函数，相似性的计算逻辑更改时修改此处。如新添加了相似性计算函数\n",
    "    :param origin_sample:\n",
    "    :param new_sample:\n",
    "    :return:\n",
    "    '''\n",
    "    prob_origin_sample = Softmax(origin_sample + 1e-7)\n",
    "    prob_new_sample = Softmax(new_sample + 1e-7)\n",
    "\n",
    "    kl = KLDivergence(prob_origin_sample,prob_new_sample)\n",
    "\n",
    "    # js = JSDivergence(prob_origin_sample,prob_new_sample)\n",
    "\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10db5b50-50a0-4eed-b7ae-e9eb97edc69e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sampleAndMatch(dataset,old_filename,new_filename,method_list,sample_num = 100,threshold = 0.5):\n",
    "    config = getConfigs()\n",
    "    # print(\"sample - dataset:\", dataset)\n",
    "    dataset_old_path = \"./RecomData/old/\" + dataset + \"/window/test/\" + old_filename + \".npy\"\n",
    "    dataset_new_path = \"./RecomData/new/\" + dataset + \"/window/test/\" + new_filename + \".npy\"\n",
    "    dataset_old_label_path = \"./RecomData/old/\" + dataset + \"/window/label/\" + old_filename + \".npy\"\n",
    "\n",
    "\n",
    "    old_window_data = np.load(dataset_old_path)\n",
    "    new_window_data = np.load(dataset_new_path)\n",
    "\n",
    "    old_data_length = int(len(old_window_data)*0.95)\n",
    "\n",
    "\n",
    "    old_label_data = np.load(dataset_old_label_path)\n",
    "\n",
    "\n",
    "    old_window_data = old_window_data[-old_data_length:]\n",
    "    new_window_data = new_window_data[-old_data_length:]\n",
    "    old_label_data = old_label_data[-old_data_length:]\n",
    "    \n",
    "    old_window_samples,old_indices = sampleFromWindowData(old_window_data,sample_num)\n",
    "    new_window_samples,new_indices = sampleFromWindowData(new_window_data,sample_num)\n",
    "    old_label_samples,new_indices = sampleFromWindowData(old_label_data,sample_num)\n",
    "\n",
    "\n",
    "    # print(\"new dataset . len: \", len(new_window_samples),\" shape:\",old_window_samples[0].shape)\n",
    "\n",
    "    # new_window_samples = unique(new_window_samples)\n",
    "\n",
    "    method_recommond_score = []\n",
    "    method_score_map = {}\n",
    "    all_params = getEvalCount(mode=\"old\", dataset=dataset, method_list=method_list)\n",
    "    anomaly_ratio = np.sum(old_label_data)/(len(old_label_data) * config[\"window_size\"])\n",
    "    for method in method_list:\n",
    "\n",
    "        score_path = \"./RecomData/scores/old/\" + dataset + \"/\" + old_filename + \"/\" + method + \".npy\"\n",
    "        anomaly_scores = np.load(score_path)\n",
    "        anomaly_scores = anomaly_scores[:, np.newaxis]\n",
    "\n",
    "        anomaly_scores = convertToWindow(anomaly_scores,config[\"window_size\"]).squeeze()\n",
    "   \n",
    "        anomaly_scores = anomaly_scores[-old_data_length:]\n",
    "        anomaly_scores_samples,_ = sampleFromWindowData(anomaly_scores,sample_num,indices=old_indices)\n",
    "        #old_label_samples,_ = sampleFromWindowData(old_label_data,sample_num,indices=old_indices)\n",
    "\n",
    "        params = all_params[dataset][old_filename][method]\n",
    "        params[\"anomaly_ratio\"] = anomaly_ratio\n",
    "        total_dataset_recommond_score = getDatasetSimilarity(old_window_samples,new_window_samples,old_anomaly_scores=anomaly_scores_samples,old_label_samples = old_label_samples,threshold = threshold,params=params)\n",
    "        # print(\"method:\",method,\" score:\",total_dataset_recommond_score)\n",
    "        method_score_map[method] = total_dataset_recommond_score\n",
    "        method_recommond_score.append(total_dataset_recommond_score)\n",
    "\n",
    "\n",
    "    max_score_index = np.array(method_recommond_score).argmax(axis=0)\n",
    "    max_score = np.array(method_recommond_score).max(axis=0)\n",
    "    method_score_map = dict(sorted(method_score_map.items(), key=lambda x: x[1], reverse=True))\n",
    "    recommon_method = method_list[max_score_index]\n",
    "    return recommon_method,max_score,method_score_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5adfd57-7f1f-4188-8d87-fc12adb60f6b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def bordaAggregation(rank_list,method_list):\n",
    "\n",
    "    \n",
    "    file_list = []\n",
    "    for item in rank_list:\n",
    "        sorted_res = sorted(item.items(), key=lambda x: x[1], reverse=True)\n",
    "        mapt = {}\n",
    "        index = 1\n",
    "\n",
    "        for item in sorted_res:\n",
    "        \n",
    "            mapt[item[0]] = index\n",
    "            index += 1\n",
    "\n",
    "        file_list.append(mapt)\n",
    "\n",
    "    \n",
    "    rank_map = {\n",
    "    }\n",
    "\n",
    "    for method in method_list:\n",
    "        if rank_map.get(method) == None:\n",
    "            rank_map[method] = []\n",
    "\n",
    "        for rank_file in file_list:\n",
    "            rank_map[method].append(rank_file[method])\n",
    "\n",
    "\n",
    "    num_competitors = len(rank_map.keys())\n",
    "\n",
    "    scores = {method: 0 for method in method_list}\n",
    "\n",
    "    for competitor, ranks in rank_map.items():\n",
    "        for rank in ranks:\n",
    "            scores[competitor] += (num_competitors - rank)\n",
    "\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8220e1ca-d472-4423-b373-29410272c93b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def recommendAll(dataset_list=[], method_list = [\"LSTMVAE\",\"LSTMAE\",\"NASALSTM\",\"DAGMM\",\"TRANSFORMER\",\"TCNAE\",\"UAE\",\"TRANAD\",\"OmniAnomaly\",\"PCAAD\",\"IForestAD\"]):\n",
    "\n",
    "\n",
    "    file_recommond_method_list = []\n",
    "\n",
    "    dataset_recommond_rank = {}\n",
    "\n",
    "    for dataset,isonly in dataset_list:\n",
    "        print(\"recommending dataset:\",dataset)\n",
    "        if isonly:\n",
    "            old_filename = dataset\n",
    "            new_filename = dataset\n",
    "            recommond_method,max_score,rank_map = sampleAndMatch(dataset,old_filename=old_filename,new_filename=new_filename,method_list=method_list,sample_num=100,threshold = 0.05)\n",
    "            file_recommond_method_list.append((dataset, dataset , recommond_method))\n",
    "            dataset_recommond_rank[dataset] = rank_map\n",
    "            print(\"recommond method:\", recommond_method)\n",
    "            print(\"method rank:\",rank_map)\n",
    "        else:\n",
    "\n",
    "\n",
    "            old_data_path = \"./RecomData/old/\" + dataset + \"/window/test/\"\n",
    "            new_data_path = \"./RecomData/new/\" + dataset + \"/window/test/\"\n",
    "\n",
    "            old_data_files = os.listdir(old_data_path)\n",
    "            new_data_files = os.listdir(new_data_path)\n",
    "\n",
    "            file_recommond_rank_list = []\n",
    "            for new_filename in new_data_files:\n",
    "                file_recommond_rank_map = {}\n",
    "                print(\"new_filename:\",new_filename)\n",
    "                total_rec_method = \"\"\n",
    "                total_max_score = 0\n",
    "                for old_filename in old_data_files:\n",
    "                    # print(\"old_filename:\", old_filename)\n",
    "                    recommond_method, max_score,rank_map = sampleAndMatch(dataset, old_filename=old_filename.split(\".\")[0],\n",
    "                                                                new_filename=new_filename.split(\".\")[0], method_list=method_list,\n",
    "                                                                sample_num=100,threshold = 0.05)\n",
    "\n",
    "                    for (method,score) in rank_map.items():\n",
    "                        if file_recommond_rank_map.get(method) == None:\n",
    "                            file_recommond_rank_map[method] = score\n",
    "                        else:\n",
    "                            file_recommond_rank_map[method] = max(file_recommond_rank_map[method],score)\n",
    "\n",
    "                    # print(\"recommond method:\",recommond_method)\n",
    "                    if max_score > total_max_score:\n",
    "                        total_max_score = max_score\n",
    "                        total_rec_method = recommond_method\n",
    "\n",
    "                file_recommond_rank_list.append(file_recommond_rank_map)\n",
    "                file_recommond_method_list.append((dataset,new_filename,total_rec_method))\n",
    "\n",
    "            aggratated_rank = bordaAggregation(file_recommond_rank_list,method_list)\n",
    "            dataset_recommond_rank[dataset] = aggratated_rank\n",
    "    print(\"final result:\")\n",
    "    print(file_recommond_method_list)\n",
    "    print(\"dataset_recommond_rank：\\n\",dataset_recommond_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702667de-d6b8-4ed0-99e2-825fdf8f1552",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e090a975-80f8-4fe3-81bb-7302f7ca3a9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: SMAP\n",
      "new_filename: A-1.npy\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precisionc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[85], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrecommendAll\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSMAP\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[84], line 35\u001B[0m, in \u001B[0;36mrecommendAll\u001B[0;34m(dataset_list, method_list)\u001B[0m\n\u001B[1;32m     32\u001B[0m total_max_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m old_filename \u001B[38;5;129;01min\u001B[39;00m old_data_files:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# print(\"old_filename:\", old_filename)\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m     recommond_method, max_score,rank_map \u001B[38;5;241m=\u001B[39m \u001B[43msampleAndMatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mold_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mold_filename\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43mnew_filename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_filename\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod_list\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43msample_num\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (method,score) \u001B[38;5;129;01min\u001B[39;00m rank_map\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m     40\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m file_recommond_rank_map\u001B[38;5;241m.\u001B[39mget(method) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[82], line 49\u001B[0m, in \u001B[0;36msampleAndMatch\u001B[0;34m(dataset, old_filename, new_filename, method_list, sample_num, threshold)\u001B[0m\n\u001B[1;32m     47\u001B[0m params \u001B[38;5;241m=\u001B[39m all_params[dataset][old_filename][method]\n\u001B[1;32m     48\u001B[0m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manomaly_ratio\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m anomaly_ratio\n\u001B[0;32m---> 49\u001B[0m total_dataset_recommond_score \u001B[38;5;241m=\u001B[39m \u001B[43mgetDatasetSimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mold_window_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnew_window_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43mold_anomaly_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43manomaly_scores_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43mold_label_samples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mold_label_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m# print(\"method:\",method,\" score:\",total_dataset_recommond_score)\u001B[39;00m\n\u001B[1;32m     51\u001B[0m method_score_map[method] \u001B[38;5;241m=\u001B[39m total_dataset_recommond_score\n",
      "Cell \u001B[0;32mIn[78], line 56\u001B[0m, in \u001B[0;36mgetDatasetSimilarity\u001B[0;34m(origin_sample_list, new_sample_list, old_anomaly_scores, old_label_samples, threshold, params)\u001B[0m\n\u001B[1;32m     51\u001B[0m q \u001B[38;5;241m=\u001B[39m Softmax(q \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-8\u001B[39m)\n\u001B[1;32m     52\u001B[0m dataset_similarity \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m (p \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(p\u001B[38;5;241m/\u001B[39mq) \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1e-8\u001B[39m)\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m---> 56\u001B[0m m_dec_total \u001B[38;5;241m=\u001B[39m \u001B[43mgetDatasetDetectability\u001B[49m\u001B[43m(\u001B[49m\u001B[43morigin_sample_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_sample_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mold_anomaly_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43mold_label_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m#print(\"dataset_similarity = \", dataset_similarity,\"m_dec_total = \", m_dec_total )\u001B[39;00m\n\u001B[1;32m     58\u001B[0m total_similarity \u001B[38;5;241m=\u001B[39m dataset_similarity \u001B[38;5;241m*\u001B[39m m_dec_total\n",
      "Cell \u001B[0;32mIn[9], line 4\u001B[0m, in \u001B[0;36mgetDatasetDetectability\u001B[0;34m(origin_sample_list, new_sample_list, old_anomaly_scores, threshold, label_samples, params)\u001B[0m\n\u001B[1;32m      2\u001B[0m total_dec \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m new_index, new_sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(new_sample_list):\n\u001B[0;32m----> 4\u001B[0m     m_dec \u001B[38;5;241m=\u001B[39m \u001B[43mgetMatchScore\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnew_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43mori_sample_list\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43morigin_sample_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[43m,\u001B[49m\u001B[43manomaly_score\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mold_anomaly_scores\u001B[49m\u001B[43m,\u001B[49m\u001B[43mlabel_samples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabel_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     total_dec \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m m_dec\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_dec\n",
      "Cell \u001B[0;32mIn[77], line 10\u001B[0m, in \u001B[0;36mgetMatchScore\u001B[0;34m(sample, ori_sample_list, threshold, anomaly_score, label_samples, params)\u001B[0m\n\u001B[1;32m      8\u001B[0m     similarity \u001B[38;5;241m=\u001B[39m getSimilarity(sample, ori_sample)\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m similarity \u001B[38;5;241m<\u001B[39m threshold:\n\u001B[0;32m---> 10\u001B[0m         m_dec \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m  np\u001B[38;5;241m.\u001B[39msum( (\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[43mprecisionc\u001B[49m)) \u001B[38;5;241m*\u001B[39m  np\u001B[38;5;241m.\u001B[39mmultiply(label_samples[index],anomaly_score[index] ) \u001B[38;5;241m+\u001B[39m ratio \u001B[38;5;241m*\u001B[39m recall \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mmultiply((\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m label_samples[index]),(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39manomaly_score[index])) )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m m_dec\n",
      "\u001B[0;31mNameError\u001B[0m: name 'precisionc' is not defined"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"SMAP\", False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb768c1a-cd02-4235-889f-d826b7663200",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: SMD\n",
      "new_filename: machine-1-4.npy\n",
      "new_filename: machine-1-5.npy\n",
      "new_filename: machine-2-1.npy\n",
      "new_filename: machine-2-3.npy\n",
      "new_filename: machine-3-1.npy\n",
      "new_filename: machine-3-11.npy\n",
      "new_filename: machine-3-3.npy\n",
      "new_filename: machine-3-5.npy\n",
      "new_filename: machine-3-9.npy\n",
      "final result:\n",
      "[('SMD', 'machine-1-4.npy', 'UAE'), ('SMD', 'machine-1-5.npy', 'NASALSTM'), ('SMD', 'machine-2-1.npy', 'UAE'), ('SMD', 'machine-2-3.npy', 'NASALSTM'), ('SMD', 'machine-3-1.npy', 'UAE'), ('SMD', 'machine-3-11.npy', 'LSTMAE'), ('SMD', 'machine-3-3.npy', 'UAE'), ('SMD', 'machine-3-5.npy', 'UAE'), ('SMD', 'machine-3-9.npy', 'UAE')]\n",
      "dataset_recommond_rank：\n",
      " {'SMD': [('UAE', 85), ('NASALSTM', 74), ('LSTMAE', 69), ('TRANSFORMER', 65), ('DAGMM', 63), ('TRANAD', 42), ('OmniAnomaly', 37), ('LSTMVAE', 26), ('IForestAD', 22), ('TCNAE', 12), ('PCAAD', 0)]}\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"SMD\", False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675add5e-7cbf-4035-984a-542b894dccdf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "recommendAll( [ (\"MSL\", False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6807f04c-48e6-4958-a3cd-a7c5605602f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: SKAB\n",
      "recommond method: PCAAD\n",
      "method rank: {'PCAAD': 17315103628.946133, 'TCNAE': 16953149395.95665, 'NASALSTM': 15997729270.00425, 'IForestAD': 15697722965.585575, 'TRANAD': 15634557788.526241, 'LSTMVAE': 15106138690.00208, 'OmniAnomaly': 14462670550.28477, 'DAGMM': 13641291104.453764, 'LSTMAE': 13496196999.90109, 'TRANSFORMER': 11216309112.121893, 'UAE': 490474526.1052186}\n",
      "final result:\n",
      "[('SKAB', 'SKAB', 'PCAAD')]\n",
      "dataset_recommond_rank：\n",
      " {'SKAB': {'PCAAD': 17315103628.946133, 'TCNAE': 16953149395.95665, 'NASALSTM': 15997729270.00425, 'IForestAD': 15697722965.585575, 'TRANAD': 15634557788.526241, 'LSTMVAE': 15106138690.00208, 'OmniAnomaly': 14462670550.28477, 'DAGMM': 13641291104.453764, 'LSTMAE': 13496196999.90109, 'TRANSFORMER': 11216309112.121893, 'UAE': 490474526.1052186}}\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"SKAB\", True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "01428caf-4aa3-4712-a154-24f56b2e0600",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: DMDS\n",
      "recommond method: IForestAD\n",
      "method rank: {'IForestAD': 13767398.373466793, 'PCAAD': 13127275.657074738, 'TRANAD': 11857608.523416948, 'LSTMAE': 11456702.423884919, 'OmniAnomaly': 11078572.008303167, 'LSTMVAE': 10936071.207215384, 'TRANSFORMER': 8217453.228245157, 'UAE': 5903162.588474636, 'NASALSTM': 5793741.948019818, 'DAGMM': 2444440.6097118724, 'TCNAE': 118077.69491741875}\n",
      "final result:\n",
      "[('DMDS', 'DMDS', 'IForestAD')]\n",
      "dataset_recommond_rank：\n",
      " {'DMDS': {'IForestAD': 13767398.373466793, 'PCAAD': 13127275.657074738, 'TRANAD': 11857608.523416948, 'LSTMAE': 11456702.423884919, 'OmniAnomaly': 11078572.008303167, 'LSTMVAE': 10936071.207215384, 'TRANSFORMER': 8217453.228245157, 'UAE': 5903162.588474636, 'NASALSTM': 5793741.948019818, 'DAGMM': 2444440.6097118724, 'TCNAE': 118077.69491741875}}\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"DMDS\", True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29809a84-8ec4-46f1-8200-dae9ca977daf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: SWAT\n",
      "recommond method: DAGMM\n",
      "method rank: {'DAGMM': 2464479318.4481654, 'NASALSTM': 1040146779.0676414, 'IForestAD': 1033167473.6583552, 'TRANAD': 933925409.488376, 'LSTMAE': 902566400.4091152, 'OmniAnomaly': 839181034.166602, 'LSTMVAE': 731146111.8301853, 'TRANSFORMER': 680397177.8311994, 'PCAAD': 31309562.566193733, 'UAE': 22615642.807857543, 'TCNAE': 947122.9120385257}\n",
      "final result:\n",
      "[('SWAT', 'SWAT', 'DAGMM')]\n",
      "dataset_recommond_rank：\n",
      " {'SWAT': {'DAGMM': 2464479318.4481654, 'NASALSTM': 1040146779.0676414, 'IForestAD': 1033167473.6583552, 'TRANAD': 933925409.488376, 'LSTMAE': 902566400.4091152, 'OmniAnomaly': 839181034.166602, 'LSTMVAE': 731146111.8301853, 'TRANSFORMER': 680397177.8311994, 'PCAAD': 31309562.566193733, 'UAE': 22615642.807857543, 'TCNAE': 947122.9120385257}}\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"SWAT\", True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bda2f597-d2e0-435c-90a8-b9d92aaae551",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: WADI\n",
      "recommond method: UAE\n",
      "method rank: {'UAE': 205966638.23271486, 'TRANSFORMER': 113315331.73608245, 'DAGMM': 101476915.70523307, 'NASALSTM': 100112316.23169225, 'IForestAD': 82111716.61109865, 'TRANAD': 54061142.07926776, 'OmniAnomaly': 47044465.54148892, 'LSTMVAE': 40250208.280937515, 'LSTMAE': 32357387.250130493, 'PCAAD': 15215633.425950002, 'TCNAE': 151.5246156344774}\n",
      "final result:\n",
      "[('WADI', 'WADI', 'UAE')]\n",
      "dataset_recommond_rank：\n",
      " {'WADI': {'UAE': 205966638.23271486, 'TRANSFORMER': 113315331.73608245, 'DAGMM': 101476915.70523307, 'NASALSTM': 100112316.23169225, 'IForestAD': 82111716.61109865, 'TRANAD': 54061142.07926776, 'OmniAnomaly': 47044465.54148892, 'LSTMVAE': 40250208.280937515, 'LSTMAE': 32357387.250130493, 'PCAAD': 15215633.425950002, 'TCNAE': 151.5246156344774}}\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"WADI\", True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f675c15e-1cb7-43e0-ab04-7df7391c3151",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: PMS\n",
      "recommond method: NASALSTM\n",
      "method rank: {'NASALSTM': 21073761579.18816, 'IForestAD': 20096481574.073124, 'DAGMM': 19533417002.501072, 'LSTMVAE': 19087763374.456913, 'OmniAnomaly': 19012448300.09833, 'LSTMAE': 18303107717.916965, 'TRANSFORMER': 17834676261.031807, 'TRANAD': 16347525642.169195, 'TCNAE': 2712111469.1037116, 'UAE': 836563731.5105536, 'PCAAD': 301296417.6968529}\n",
      "final result:\n",
      "[('PMS', 'PMS', 'NASALSTM')]\n",
      "dataset_recommond_rank：\n",
      " {'PMS': {'NASALSTM': 21073761579.18816, 'IForestAD': 20096481574.073124, 'DAGMM': 19533417002.501072, 'LSTMVAE': 19087763374.456913, 'OmniAnomaly': 19012448300.09833, 'LSTMAE': 18303107717.916965, 'TRANSFORMER': 17834676261.031807, 'TRANAD': 16347525642.169195, 'TCNAE': 2712111469.1037116, 'UAE': 836563731.5105536, 'PCAAD': 301296417.6968529}}\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"PMS\", True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e86260-3317-45f9-902a-556e17d9f20e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommending dataset: UCR\n",
      "new_filename: 115_UCR_Anomaly_CIMIS44AirTemperature3_4000_6520_6544.npy\n",
      "new_filename: 001_UCR_Anomaly_DISTORTED1sddb40_35000_52000_52620.npy\n",
      "new_filename: 063_UCR_Anomaly_DISTORTEDgaitHunt2_18500_31200_31850.npy\n",
      "new_filename: 006_UCR_Anomaly_DISTORTEDCIMIS44AirTemperature2_4000_5703_5727.npy\n",
      "new_filename: 075_UCR_Anomaly_DISTORTEDqtdbSel100MLII_4000_13400_13800.npy\n",
      "new_filename: 007_UCR_Anomaly_DISTORTEDCIMIS44AirTemperature3_4000_6520_6544.npy\n",
      "new_filename: 070_UCR_Anomaly_DISTORTEDltstdbs30791AI_17555_52600_52800.npy\n",
      "new_filename: 136_UCR_Anomaly_InternalBleeding17_1600_3198_3309.npy\n",
      "new_filename: 015_UCR_Anomaly_DISTORTEDECG4_5000_16800_17100.npy\n",
      "new_filename: 021_UCR_Anomaly_DISTORTEDGP711MarkerLFM5z3_5000_5948_5993.npy\n",
      "new_filename: 026_UCR_Anomaly_DISTORTEDInternalBleeding15_1700_5684_5854.npy\n",
      "new_filename: 099_UCR_Anomaly_NOISEInternalBleeding6_1500_3474_3629.npy\n",
      "new_filename: 032_UCR_Anomaly_DISTORTEDInternalBleeding4_1000_4675_5033.npy\n",
      "new_filename: 142_UCR_Anomaly_InternalBleeding6_1500_3474_3629.npy\n",
      "new_filename: 250_UCR_Anomaly_weallwalk_2951_7290_7296.npy\n",
      "new_filename: 081_UCR_Anomaly_DISTORTEDresperation3_45000_158250_158251.npy\n",
      "new_filename: 143_UCR_Anomaly_InternalBleeding8_2500_5865_5974.npy\n",
      "new_filename: 085_UCR_Anomaly_DISTORTEDs20101m_10000_35774_35874.npy\n",
      "new_filename: 062_UCR_Anomaly_DISTORTEDgaitHunt1_18500_33070_33180.npy\n",
      "new_filename: 090_UCR_Anomaly_DISTORTEDtiltAPB2_50000_124159_124985.npy\n",
      "new_filename: 013_UCR_Anomaly_DISTORTEDECG3_15000_16000_16100.npy\n",
      "new_filename: 078_UCR_Anomaly_DISTORTEDresperation1_100000_110260_110412.npy\n",
      "new_filename: 025_UCR_Anomaly_DISTORTEDInternalBleeding14_2800_5607_5634.npy\n",
      "new_filename: 028_UCR_Anomaly_DISTORTEDInternalBleeding17_1600_3198_3309.npy\n",
      "new_filename: 105_UCR_Anomaly_NOISEgait3_24500_59900_60500.npy\n",
      "new_filename: 118_UCR_Anomaly_CIMIS44AirTemperature6_4000_6006_6054.npy\n",
      "new_filename: 104_UCR_Anomaly_NOISEapneaecg4_6000_16000_16100.npy\n",
      "new_filename: 029_UCR_Anomaly_DISTORTEDInternalBleeding18_2300_4485_4587.npy\n",
      "new_filename: 009_UCR_Anomaly_DISTORTEDCIMIS44AirTemperature5_4000_4852_4900.npy\n"
     ]
    }
   ],
   "source": [
    "recommendAll( [ (\"UCR\", False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d232b-ec38-4d58-b636-e362cd55086c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2835f4-91cf-4743-ba01-56314b233319",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[('SKAB', 'SKAB.npy', 'PCAAD'), ('PMS', 'PMS.npy', 'IForestAD'), ('DMDS', 'DMDS.npy', 'LSTMV2'), ('WADI', 'WADI.npy', 'UAE'), ('SWAT', 'SWAT.npy', 'DAGMM')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7e546-4e1b-4ecb-a1ec-04c9e62073d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"filename: SWAT\"\n",
    "[('IForestAD', 0.7213506065143328), ('TRANSFORMER', 0.7185557408650827), ('OmniAnomaly', 0.7110767872903795), ('LSTMV2', 0.7110390326780079), ('TRANAD', 0.7109850825315562), ('LSTMAE', 0.7108247650147831), ('LSTMVAE', 0.7071683880446462), ('DAGMM', 0.15338251291813979), ('TCNAE', 0.013140139365114479), ('PCAAD', 0.001061266001901435), ('UAE', 0.0004913794945200505)]\n",
    "\n",
    "\"filename: WADI\"\n",
    "[('UAE', 0.2995198079231693), ('OmniAnomaly', 0.29505524361528707), ('IForestAD', 0.2552675264972545), ('LSTMAE', 0.2483051973946564), ('TRANSFORMER', 0.24186666666666667), ('LSTMV2', 0.20349990979613927), ('TRANAD', 0.18125061936378953), ('DAGMM', 0.17701307639366828), ('LSTMVAE', 0.16818130820150343), ('PCAAD', 0.021135265700483092), ('TCNAE', 0.0)]\n",
    "\n",
    "\n",
    "\"DMDS\"\n",
    "[('DAGMM', 0.7722356557761157), ('TRANAD', 0.7212254570074476), ('OmniAnomaly', 0.7206452377325276), ('LSTMV2', 0.7205046678392639), ('LSTMVAE', 0.7203857215361191), ('LSTMAE', 0.7202058504875406), ('IForestAD', 0.7200270407300997), ('TRANSFORMER', 0.7200027062683941), ('UAE', 0.7170389170896785), ('PCAAD', 0.6552801127947029), ('TCNAE', 0.05941940627491441)]\n",
    "\n",
    "\"SKAB\"\n",
    "[('TCNAE', 0.41014799154334036), ('IForestAD', 0.3656400966183575), ('LSTMV2', 0.3520237653174898), ('PCAAD', 0.35197400807324997), ('LSTMAE', 0.35126818351361433), ('OmniAnomaly', 0.3498418436511523), ('LSTMVAE', 0.34839650145772594), ('TRANSFORMER', 0.34461649261793303), ('TRANAD', 0.3414611955236102), ('DAGMM', 0.3378646020885848), ('UAE', 0.09733270940570894)]\n",
    "\n",
    "\n",
    "\"PMS\"\n",
    "[('IForestAD', 0.41203598699629546), ('DAGMM', 0.3917796550010648), ('TRANSFORMER', 0.36127988275883127), ('TRANAD', 0.3565100154083205), ('LSTMVAE', 0.3561535417491096), ('LSTMAE', 0.35247352566690476), ('LSTMV2', 0.351559528337771), ('OmniAnomaly', 0.31978100600367065), ('UAE', 0.005663164806303349), ('TCNAE', 0.0024529844644317253), ('PCAAD', 0.0011495196649971263)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d124b-3b72-4298-bb13-7d4beedd5705",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"filename: WADI\"\n",
    "[('UAE', 0.2995198079231693), ('OmniAnomaly', 0.29505524361528707), ('IForestAD', 0.2552675264972545), ('LSTMAE', 0.2483051973946564), ('TRANSFORMER', 0.24186666666666667), ('LSTMV2', 0.20349990979613927), ('TRANAD', 0.18125061936378953), ('DAGMM', 0.17701307639366828), ('LSTMVAE', 0.16818130820150343), ('PCAAD', 0.021135265700483092), ('TCNAE', 0.0)]\n",
    "\"filename: SWAT\"\n",
    "[('IForestAD', 0.7213506065143328), ('TRANSFORMER', 0.7185557408650827), ('OmniAnomaly', 0.7110767872903795), ('LSTMV2', 0.7110390326780079), ('TRANAD', 0.7109850825315562), ('LSTMAE', 0.7108247650147831), ('LSTMVAE', 0.7071683880446462), ('DAGMM', 0.15338251291813979), ('TCNAE', 0.013140139365114479), ('PCAAD', 0.001061266001901435), ('UAE', 0.0004913794945200505)]\n",
    "\n",
    "\"filename: machine-1-4\"\n",
    "[('TRANAD', 0.21764705882352942), ('TRANSFORMER', 0.17973231357552583), ('DAGMM', 0.11703056768558952), ('LSTMVAE', 0.11320754716981132), ('LSTMV2', 0.11203319502074689), ('LSTMAE', 0.09489993544222079), ('UAE', 0.09379310344827586), ('OmniAnomaly', 0.08548168249660787), ('IForestAD', 0.061733128834355826), ('PCAAD', 0.0067842605156037995), ('TCNAE', 0.0065830721003134795)]\n",
    "\"filename: machine-1-5\"\n",
    "[('UAE', 0.23140495867768596), ('DAGMM', 0.08791208791208792), ('TRANAD', 0.0847457627118644), ('LSTMAE', 0.07936507936507936), ('LSTMVAE', 0.07926829268292683), ('OmniAnomaly', 0.06896551724137931), ('LSTMV2', 0.06692913385826772), ('TRANSFORMER', 0.02663622526636225), ('PCAAD', 0.017391304347826087), ('IForestAD', 0.006996268656716418), ('TCNAE', 0.004074455309658604)]\n",
    "\"filename: machine-2-1\"\n",
    "[('TRANSFORMER', 0.21645796064400716), ('OmniAnomaly', 0.17314487632508835), ('DAGMM', 0.16447368421052633), ('LSTMAE', 0.16012596899224807), ('LSTMV2', 0.15712545676004872), ('LSTMVAE', 0.15653864851725816), ('TRANAD', 0.15287128712871287), ('IForestAD', 0.12073649260488983), ('TCNAE', 0.037011205653973124), ('UAE', 0.02132895816242822), ('PCAAD', 0.0076077768385460695)]\n",
    "\"filename: machine-2-3\"\n",
    "[('DAGMM', 0.411371237458194), ('LSTMAE', 0.30517711171662126), ('LSTMVAE', 0.2994652406417112), ('TRANAD', 0.282798833819242), ('TRANSFORMER', 0.2671009771986971), ('LSTMV2', 0.2607655502392344), ('OmniAnomaly', 0.21194029850746268), ('IForestAD', 0.15311004784688995), ('TCNAE', 0.1505016722408027), ('UAE', 0.0037174721189591076), ('PCAAD', 0.0036101083032490976)]\n",
    "\"filename: machine-3-1\"\n",
    "[('DAGMM', 0.33116883116883117), ('IForestAD', 0.2434017595307918), ('TRANSFORMER', 0.18155619596541786), ('OmniAnomaly', 0.13527851458885942), ('PCAAD', 0.10862619808306709), ('LSTMVAE', 0.07706093189964158), ('TRANAD', 0.05252918287937743), ('LSTMV2', 0.05206463195691203), ('LSTMAE', 0.04212454212454213), ('UAE', 0.016233766233766232), ('TCNAE', 0.008375610223030535)]\n",
    "\"filename: machine-3-11\"\n",
    "[('TRANAD', 0.32019704433497537), ('LSTMV2', 0.21052631578947367), ('LSTMAE', 0.19158878504672897), ('OmniAnomaly', 0.1588785046728972), ('TCNAE', 0.1553784860557769), ('TRANSFORMER', 0.12745098039215685), ('IForestAD', 0.06304728546409807), ('UAE', 0.06), ('LSTMVAE', 0.04742268041237113), ('PCAAD', 0.03773584905660377), ('DAGMM', 0.007414619532654284)]\n",
    "\"filename: machine-3-3\"\n",
    "[('LSTMV2', 0.16666666666666666), ('LSTMAE', 0.15911485774499473), ('TRANAD', 0.15803814713896458), ('OmniAnomaly', 0.13374125874125875), ('PCAAD', 0.11435523114355231), ('TRANSFORMER', 0.07482993197278912), ('UAE', 0.06919945725915876), ('TCNAE', 0.05151737640724425), ('IForestAD', 0.038204225352112674), ('LSTMVAE', 0.03145004174784303), ('DAGMM', 0.02860680307572006)]\n",
    "\"filename: machine-3-5\"\n",
    "[('IForestAD', 0.12863070539419086), ('TRANSFORMER', 0.09896907216494845), ('DAGMM', 0.0473186119873817), ('LSTMAE', 0.0472972972972973), ('LSTMVAE', 0.04612850082372323), ('TRANAD', 0.041469194312796206), ('OmniAnomaly', 0.040893193435566313), ('TCNAE', 0.03694102397926118), ('LSTMV2', 0.03474903474903475), ('UAE', 0.02112676056338028), ('PCAAD', 0.009174311926605505)]\n",
    "\"filename: machine-3-9\"\n",
    "[('TRANSFORMER', 0.27735368956743), ('DAGMM', 0.2553846153846154), ('LSTMAE', 0.18309859154929578), ('IForestAD', 0.18154761904761904), ('LSTMV2', 0.1751269035532995), ('TRANAD', 0.1673728813559322), ('OmniAnomaly', 0.1650485436893204), ('PCAAD', 0.1355421686746988), ('LSTMVAE', 0.10256410256410256), ('UAE', 0.0297029702970297), ('TCNAE', 0.017022296811316232)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc0186-027c-4743-b4af-2e91eea532c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03641bee-d58a-4b7d-b563-7265c2da9819",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511f1ac-c64d-4b75-a0ee-aa6db0873039",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}